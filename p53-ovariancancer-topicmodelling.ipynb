{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Topic Modelling for P53 in Ovarian Cancer**\n\n\n\n\n\n\n","metadata":{"id":"YCkSAs8V07Tx"}},{"cell_type":"code","source":"!pip install bertopic","metadata":{"id":"jxfEey4ANz7v","outputId":"00b2b831-4e1c-4b1a-a822-53e71d826ec3","execution":{"iopub.status.busy":"2022-11-23T22:35:00.601914Z","iopub.execute_input":"2022-11-23T22:35:00.602287Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Collecting bertopic\n  Downloading bertopic-0.12.0-py2.py3-none-any.whl (90 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.7/90.7 KB\u001b[0m \u001b[31m963.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm>=4.41.1 in /opt/conda/lib/python3.7/site-packages (from bertopic) (4.63.0)\nRequirement already satisfied: numpy>=1.20.0 in /opt/conda/lib/python3.7/site-packages (from bertopic) (1.21.6)\nRequirement already satisfied: umap-learn>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from bertopic) (0.5.3)\nRequirement already satisfied: plotly>=4.7.0 in /opt/conda/lib/python3.7/site-packages (from bertopic) (5.8.0)\nCollecting hdbscan>=0.8.28\n  Downloading hdbscan-0.8.29.tar.gz (5.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: scikit-learn>=0.22.2.post1 in /opt/conda/lib/python3.7/site-packages (from bertopic) (1.0.2)\nCollecting pyyaml<6.0\n  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m636.6/636.6 KB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pandas>=1.1.5 in /opt/conda/lib/python3.7/site-packages (from bertopic) (1.3.5)\nCollecting sentence-transformers>=0.4.1\n  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 KB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: cython>=0.27 in /opt/conda/lib/python3.7/site-packages (from hdbscan>=0.8.28->bertopic) (0.29.28)\nRequirement already satisfied: scipy>=1.0 in /opt/conda/lib/python3.7/site-packages (from hdbscan>=0.8.28->bertopic) (1.7.3)\nRequirement already satisfied: joblib>=1.0 in /opt/conda/lib/python3.7/site-packages (from hdbscan>=0.8.28->bertopic) (1.0.1)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.1.5->bertopic) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.1.5->bertopic) (2021.3)\nRequirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from plotly>=4.7.0->bertopic) (8.0.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.1.0)\nRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers>=0.4.1->bertopic) (4.18.0)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers>=0.4.1->bertopic) (1.11.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from sentence-transformers>=0.4.1->bertopic) (0.12.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (from sentence-transformers>=0.4.1->bertopic) (3.2.4)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from sentence-transformers>=0.4.1->bertopic) (0.1.96)\nRequirement already satisfied: huggingface-hub>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers>=0.4.1->bertopic) (0.5.1)\nRequirement already satisfied: pynndescent>=0.5 in /opt/conda/lib/python3.7/site-packages (from umap-learn>=0.5.0->bertopic) (0.5.6)\nRequirement already satisfied: numba>=0.49 in /opt/conda/lib/python3.7/site-packages (from umap-learn>=0.5.0->bertopic) (0.55.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (4.2.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (21.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.27.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.6.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (4.11.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (59.8.0)\nRequirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in /opt/conda/lib/python3.7/site-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (0.38.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas>=1.1.5->bertopic) (1.16.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (0.12.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2021.11.10)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (0.0.53)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->sentence-transformers>=0.4.1->bertopic) (9.0.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.0.7)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.7.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2021.10.8)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (1.26.8)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.0.12)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.3)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (8.0.4)\nBuilding wheels for collected packages: hdbscan, sentence-transformers\n  Building wheel for hdbscan (pyproject.toml) ... \u001b[?25l/","output_type":"stream"}]},{"cell_type":"code","source":"!pip install bertopic[visualization]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import re\nimport pandas as pd\nfrom bertopic import BERTopic","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"doc = pd.read_csv(\"../input/p53ova/P53inovarian.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## KEYBERT","metadata":{}},{"cell_type":"code","source":"!pip install keybert","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#\"../input/p53-text/P53_Ovarian_19_06_22.txt\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(\"../input/p53-text/P53_Ovarian_19_06_22.txt\") as file_in:\n    lines = []\n    for line in file_in:\n        lines.append(line)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"infile = open('../input/p53-text/P53_Ovarian_19_06_22.txt', 'r')  # Open the file for reading.\n\ndata = infile.read()  # Read the contents of the file.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Splitting data  into 2 text file to conserve memory space","metadata":{}},{"cell_type":"code","source":"with open(\"../input/p53-text/P53_Ovarian_19_06_22.txt\",'r') as file:\n    lines = file.readlines()\n\nwith open(\"P53_1.txt\",'w') as file:\n    for line in lines[:int(len(lines)/4)]:\n        file.write(line)\n\nwith open(\"P53_2.txt\",'w') as file:\n    for line in lines[int(len(lines)/4)]:\n        file.write(line)\n\nwith open(\"P53_3.txt\",'w') as file:\n    for line in lines[int(len(lines)/4)]:\n        file.write(line)\n\nwith open(\"P53_4.txt\",'w') as file:\n    for line in lines[:int(len(lines)/4)]:\n        file.write(line)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#./P53_1.txt\n#./P53_2.txt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"infile = open('./P53_1.txt', 'r')  # Open the file for reading.\n\ndata1 = infile.read()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"infile = open('./P53_2.txt', 'r')  # Open the file for reading.\n\ndata2 = infile.read()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"infile = open('./P53_3.txt', 'r')  # Open the file for reading.\n\ndata3 = infile.read()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"infile = open('./P53_4.txt', 'r')  # Open the file for reading.\n\ndata4 = infile.read()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keybert import KeyBERT\nkw_model = KeyBERT()\nkeywords = kw_model.extract_keywords(data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kw_model.extract_keywords(data, keyphrase_ngram_range=(1, 1), stop_words=None,top_n = 10)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kw_model.extract_keywords(data, keyphrase_ngram_range=(1, 2), stop_words=None, top_n = 20)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#keywords = kw_model.extract_keywords(data1, highlight=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#keywords = kw_model.extract_keywords(data2, highlight=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kw_model.extract_keywords(data4, keyphrase_ngram_range=(3, 3), stop_words='english',use_maxsum=True, nr_candidates=20, top_n=20)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kw_model.extract_keywords(data4, keyphrase_ngram_range=(3, 3), stop_words='english',use_mmr=True, diversity=0.7, top_n=20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#kw_model.extract_keywords(data, keyphrase_ngram_range=(3, 3), stop_words='english', use_mmr=True, diversity=0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install yake","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import yake\nfrom keybert import KeyBERT\n# Create candidates\nkw_extractor = yake.KeywordExtractor(top=50)\ncandidates = kw_extractor.extract_keywords(data)\ncandidates = [candidate[0] for candidate in candidates]\n\n# KeyBERT init\nkw_model = KeyBERT()\nkeywords = kw_model.extract_keywords(data, candidates)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kw_model = KeyBERT()\nseed_keywords = [\"ovariancancer\"]\nkeywords = kw_model.extract_keywords(data, use_mmr=True, diversity=0.1, seed_keywords=seed_keywords)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keybert import KeyBERT\nkw_model = KeyBERT(model=\"all-MiniLM-L6-v2\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\n\nsentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\nkw_model = KeyBERT(model=sentence_model)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install nltk","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport pandas as pd\nfrom bertopic import BERTopic","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nregex = \"\\\\b[0-9]{15}|[0-9]{12}\\\\b\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/p53ova/P53inovarian.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.rename(columns = {'abstract':'text'}, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#datacleaning\ndf.text = df.apply(lambda row: re.sub(r\"http\\S+\", \"\", row.text).lower(), 1)\ndf.text = df.apply(lambda row: \" \".join(filter(lambda x:x[0]!=\"@\", row.text.split())), 1)\ndf.text = df.apply(lambda row: \" \".join(re.sub(\"[^a-zA-Z]+\", \" \", row.text).split()), 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#remove words and digits\n#df['text'] = df['text'].apply(lambda x: re.sub('W*dw*','',x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#remove stopwords\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\nstop_words.add('subject')\nstop_words.add('http')\ndef remove_stopwords(text):\n    return \" \".join([word for word in str(text).split() if word not in stop_words])\ndf['text'] = df['text'].apply(lambda x: remove_stopwords(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"titles = df.text.to_list()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create model \n \np53_model = BERTopic(verbose=True)\n \n#topics, probabilities = model.fit_transform(titles)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from bertopic import BERTopic\n#p53_model = BERTopic(nr_topics=20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from bertopic import BERTopic\n#topic_model = BERTopic(min_topic_size=70, n_gram_range=(1,3), verbose=True)\n#topics,probabilties  = topic_model.fit_transform(titles)\ntopics, probabilities = p53_model.fit_transform(titles)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"freq = p53_model.get_topic_info()\nfreq.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#topic_nr = freq.iloc[2][\"Topic\"]\ntopic_nr = freq.iloc[2][\"Topic\"] # select a frequent topic\n#p53_model.get_topic(topic_nr)\np53_model.get_topic(topic_nr)\n#nr_candidates=20, top_n=20","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#p53_model.get_topic(top_n_topics=30)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#loaded_model = BERTopic.load(\"p53_model\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p53_model.get_topic_freq().head(11)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p53_model.get_topic(-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p53_model.get_topic(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = p53_model.visualize_topics(); fig","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p53_model.visualize_hierarchy(top_n_topics=30)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p53_model.visualize_heatmap()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p53_model.get_topic(6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p53_model.visualize_barchart()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"similar_topics, similarity = p53_model.find_topics(\"colorectal\", top_n = 3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"similar_topics ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"most_similar = similar_topics[0]\nprint(\"Most Similar Topic Info: \\n{}\".format(p53_model.get_topic(most_similar)))\nprint(\"Similarity Score: {}\".format(similarity[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%bash\nmkdir './cancerhealth_disparities_model_dir'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#import pickle\n \n# Save the trained model as a pickle string.\n#saved_model = pickle.dumps(cancerhealth_disparities_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p53_model.save(\"p53_model.h5\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the model in the previously created folder with the name 'my_best_model'\n#cancerhealth_disparities_model.save(\"./model_dir/my_best_model\")\n\n# Load the serialized model\nmy_best_model = BERTopic.load(\"./p53_model.h5\")\nmy_best_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loaded_model = BERTopic.load(\"p53_model.h5\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p53_model.visualize_distribution(probabilities[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install octis","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load a custom dataset","metadata":{}},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport string\nfrom octis.preprocessing.preprocessing import Preprocessing\nos.chdir(os.path.pardir)\n\n# Initialize preprocessing\npreprocessor = Preprocessing(vocabulary=None, max_features=None,\n                             remove_punctuation=True, punctuation=string.punctuation,\n                             lemmatize=True, stopword_list='english',\n                             min_chars=1, min_words_docs=0)\n# preprocess\n#../input/p53-text\n#dataset = preprocessor.preprocess_dataset(documents_path=r'..\\corpus.txt', labels_path=r'..\\labels.txt')\ndataset = preprocessor.preprocess_dataset(documents_path = '/kaggle/input/p53-text/P53_Ovarian_19_06_22.txt')\n\n# save the preprocessed dataset\ndataset.save('hello_dataset')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from octis.dataset.dataset import Dataset\nfrom octis.models.LDA import LDA\n\n# Load a dataset\ndataset = Dataset()\ndataset.load_custom_dataset_from_folder(\"dataset_folder\")\n\nmodel = LDA(num_topics=25)  # Create model\nmodel_output = model.train_model(dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install keybert","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"doc = pd.read_csv(\"../input/p53ova/P53inovarian.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keybert import KeyBERT\nkw_model = KeyBERT()\nkeywords = kw_model.extract_keywords(doc)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kw_model.extract_keywords(doc, keyphrase_ngram_range=(1, 1), stop_words=None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"JACCARD SIMILARITY SCORE","metadata":{}},{"cell_type":"code","source":"doc = \"\"\"\n         Supervised learning is the machine learning task of learning a function that\n         maps an input to an output based on example input-output pairs.[1] It infers a\n         function from labeled training data consisting of a set of training examples.[2]\n         In supervised learning, each example is a pair consisting of an input object\n         (typically a vector) and a desired output value (also called the supervisory signal).\n         A supervised learning algorithm analyzes the training data and produces an inferred function,\n         which can be used for mapping new examples. An optimal scenario will allow for the\n         algorithm to correctly determine the class labels for unseen instances. This requires\n         the learning algorithm to generalize from the training data to unseen situations in a\n         'reasonable' way (see inductive bias).\n      \"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"docu_1 = \"Ovarian cancer,Ovarian Malignancy Malignancies Pelvic Ovary Prognostication Malignance Prognoses Prognosis\"\ndocu_2 = \"Cancer Cell expression Tumor Protein Study Patient Induce Apoptosis Genes Use\"\ndocu_3= \"Cancer Mutant Tumor Mutations Type Cell Tp Protein Mdm Function\"\ndocu_4= \"Cell Cancer Expression Induce Apoptosis Study Line Level Increase Inhibit\"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def jaccard_similarity(doc1, doc2):\n \n # list unique words in the document\n words_doc1 = set(doc1.lower().split())\n words_doc2 = set(doc2.lower().split())\n \n # find the intersection of words list of doc1 & doc2\n intersection = words_doc1.intersection(words_doc2)\n \n # find the union of words list of doc1 & doc2\n union = words_doc1.union(words_doc2)\n \n # Calculate Jaccard similarity score\n # using the length of intersection set divided by the length of union set\n return float(len(intersection)) / len(union)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def jaccard_similarity(doc1, doc2, doc3, doc4):\n \n # list unique words in the document\n words_doc1 = set(doc1.lower().split())\n words_doc2 = set(doc2.lower().split())\n words_doc3 = set(doc3.lower().split()) \n words_doc4 = set(doc4.lower().split())\n \n # find the intersection of words list of doc1 & doc2\n intersection = words_doc1.intersection(words_doc2).intersection(words_doc3).intersection(words_doc4)\n \n # find the union of words list of doc1 & doc2\n union = words_doc1.union(words_doc2).union(words_doc3).union(words_doc4)\n \n # Calculate Jaccard similarity score\n # using the length of intersection set divided by the length of union set\n return float(len(intersection)) / len(union)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def jaccard_similarity(doc1, doc2, doc3):\n \n # list unique words in the document\n words_doc1 = set(doc1.lower().split())\n words_doc2 = set(doc2.lower().split())\n words_doc3 = set(doc3.lower().split())   \n \n # find the intersection of words list of doc1 & doc2\n intersection = words_doc1.intersection(words_doc2).intersection(words_doc3)\n \n # find the union of words list of doc1 & doc2\n union = words_doc1.union(words_doc2).union(words_doc3)\n \n # Calculate Jaccard similarity score\n # using the length of intersection set divided by the length of union set\n return float(len(intersection)) / len(union)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jaccard_similarity(docu_1, docu_2, docu_3,docu_4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jaccard_similarity(docu_2, docu_3,docu_4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jaccard_similarity(docu_3, docu_4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}